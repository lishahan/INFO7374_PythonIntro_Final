{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Collect Data from the Twitter Search API, store them and preprocess them\n",
    "### 1. Collect data from the Twitter Search API\n",
    "##### Cause twitter only return data from the past 7 days, I collect data everyday for the recent tweets. \n",
    "##### 100 tweets per .json file\n",
    "#####  The keywords I use is mainly about the recent UA3411 incident\n",
    "### 2. Storing Data\n",
    "##### To make sure there is no data overwrite, I saved each .json file with its query and an unix timestamp\n",
    "##### I catagorize the .json files in folders named by its keywords and they will be catagorized by time\n",
    "### 3. Preprocess Data\n",
    "##### Extract all the tweets I collect from the raw data and store them as a list in a .json file\n",
    "##### Get the information of users info, tweets, location, country and other data I will use in the future analysis and store them in a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from twitter import Twitter, OAuth, TwitterHTTPError, TwitterStream\n",
    "import time\n",
    "import datetime\n",
    "from datetime import timedelta, date\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Collect data from the Twitter Search API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "consumer_key = 'grb5AnK0mOzm6BsU7NVbG0Lz3'\n",
    "consumer_secret = 'thAUHH3fns0TE2wYAGFhC6PrUQktpFO4IAIZWDFeDPvickJAKD'\n",
    "access_token = '853426630483726336-PtJ837ZcRxu8jxzMS5WJ1ukDaNLp2NM'\n",
    "access_secret = 'ySMHkMbjXXkDhXpQWpMJX9CThtGBTOKInVMIW0LS9UirN'\n",
    "oauth = OAuth(access_token, access_secret, consumer_key, consumer_secret)\n",
    "twitter = Twitter(auth=oauth)\n",
    "path_local='/Users/lisha/Desktop/Final/Data/Data_Raw/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cause twitter only return data from the past 7 days, I collect data everyday for the recent tweets. \n",
    "##### 100 tweets per .json file\n",
    "#####  The keywords I use is mainly about the recent UA3411 incident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def date_range(start, end):\n",
    "    r = (end+datetime.timedelta(days=1)-start).days\n",
    "    return [start+datetime.timedelta(days=i) for i in range(r)] \n",
    "end = datetime.date(2017, 4, 20)\n",
    "start = end-timedelta(days=0)\n",
    "dateList = date_range(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.date(2017, 4, 20)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dateList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_key=['airline','united airline', 'overbook','3411','david dao','hare airport', 'oscar munoz','customer','flight 3411','CNN','louisville international airport','unwilling passenger','apologiz','voucher','passenger','seat','weibo','asian','beat','lawsuit','UA3411','fare','flight','compensation','violent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for searchquery in list_key:\n",
    "    for i in dateList:\n",
    "        t=twitter.search.tweets(q= searchquery, result_type='mixed', lang='en', count=100, until=i)\n",
    "        timestamp=int(time.mktime(time.strptime(str(datetime.datetime.now().replace(microsecond=0)), '%Y-%m-%d %H:%M:%S')))\n",
    "        stamp = t['search_metadata']['query']+str(timestamp)\n",
    "        if t['statuses'] != []:\n",
    "            f = open(path_local + str(stamp) + '.json',\"w\")\n",
    "            json.dump(t,f)\n",
    "            f.close()\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Storing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_jsonfiles = glob.glob(path_local + '*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dic = {}\n",
    "for file in list_jsonfiles:\n",
    "    with open(file) as data_file:\n",
    "        data_dic[file] = json.load(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To make sure there is no data overwrite, I saved each .json file with its query and an unix timestamp\n",
    "##### I catagorize the .json files in folders named by its keywords and they will be catagorized by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for directory in list_jsonfiles:\n",
    "    path_country = ''\n",
    "    path_city = ''\n",
    "    path_term = ''\n",
    "    path_key = 'Data/Data_Processed/' + directory.split('+')[0].split('/')[-1] + '/'\n",
    "    path_date = path_key + directory[-25:-15] + '/'\n",
    "    if not os.path.exists(path_key):\n",
    "        os.makedirs(path_key)\n",
    "    if not os.path.exists(path_date):\n",
    "        os.makedirs(path_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for directory in list_jsonfiles:\n",
    "    dst = ''\n",
    "    dst = 'Data/Data_Processed/' + directory.split('+')[0].split('/')[-1] + '/' + directory[-25:-15] + '/'\n",
    "    shutil.copy(directory, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocess Data\n",
    "##### Extract all the tweets I collect from the raw data and store them as a list in a .json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_tweets=[]\n",
    "for file in list_jsonfiles:\n",
    "    with open(file) as data_file:\n",
    "        d=json.load(data_file)\n",
    "        tweets=d['statuses']\n",
    "        list_temp=list(map(lambda tweet: tweet['text'], tweets))\n",
    "    list_tweets.extend(list_temp)\n",
    "f = open('/Users/lisha/Desktop/Final/Data/Data_Clean/Tweets.txt',\"w\")\n",
    "json.dump(list_tweets,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['United CEO response to United Express Flight 3411. https://t.co/rF5gNIvVd0',\n",
       " 'United CEO Oscar Munoz: I’m sorry. We will fix this. https://t.co/v8EPGsiDCi https://t.co/eOPiYcagvo',\n",
       " '@USAnonymous Flight 3411 from Chicago to Louisville was overbooked. After our team looked for volunteers, one customer refused to leave ^MD',\n",
       " 'RT @UnitedOverBooks: now boarding flight 3411 jk lol we over booked https://t.co/cT2IHKNuXe',\n",
       " 'RT @UnitedOverBooks: now boarding flight 3411 jk lol we over booked https://t.co/cT2IHKNuXe']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_tweets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get the information of users info, tweets, location, country and other data I will use in the future analysis and store them in a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_info(tweets):\n",
    "    df = pd.DataFrame()\n",
    "    df['id']= list(map(lambda tweet: tweet['id'], tweets)) \n",
    "    df['name']=list(map(lambda tweet: tweet['user']['name'], tweets))\n",
    "    df['created_at']= list(map(lambda tweet: tweet['created_at'], tweets)) \n",
    "    df['text'] = list(map(lambda tweet: tweet['text'], tweets))  \n",
    "    df['country_code'] = list(map(lambda tweet: tweet['place']['country_code'] if tweet['place'] is not None else '', tweets)) \n",
    "    df['long'] = list(map(lambda tweet: tweet['place']['bounding_box']['coordinates'][0][0][0] if tweet['place'] is not None else '', tweets)) \n",
    "    df['latt'] = list(map(lambda tweet: tweet['place']['bounding_box']['coordinates'][0][0][1]if tweet['place'] is not None else '', tweets))\n",
    "    df['retweet_count']= list(map(lambda tweet: tweet['retweet_count'], tweets))        \n",
    "    df['user_created']=list(map(lambda tweet: tweet['user']['created_at'], tweets)) \n",
    "    df['followers_count']=list(map(lambda tweet: tweet['user']['followers_count'], tweets))\n",
    "    df['friends_count'] = list(map(lambda tweet: tweet['user']['friends_count'], tweets))\n",
    "    df['user_language']=list(map(lambda tweet: tweet['user']['lang'], tweets))\n",
    "    df['favorite_count']=list(map(lambda tweet: tweet['favorite_count'], tweets)) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "list_df=[]\n",
    "for file in list_jsonfiles:\n",
    "    with open(file) as data_file:\n",
    "        df_temp=pd.DataFrame()\n",
    "        d=json.load(data_file)\n",
    "        tweets=d['statuses']\n",
    "        df_temp=get_info(tweets)\n",
    "        list_df.append(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.concat(list_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>country_code</th>\n",
       "      <th>long</th>\n",
       "      <th>latt</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>user_created</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>user_language</th>\n",
       "      <th>favorite_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851471781827420160</td>\n",
       "      <td>United</td>\n",
       "      <td>Mon Apr 10 16:27:47 +0000 2017</td>\n",
       "      <td>United CEO response to United Express Flight 3...</td>\n",
       "      <td>US</td>\n",
       "      <td>-95.8233</td>\n",
       "      <td>29.5223</td>\n",
       "      <td>21757</td>\n",
       "      <td>Fri Mar 04 21:23:11 +0000 2011</td>\n",
       "      <td>913424</td>\n",
       "      <td>46648</td>\n",
       "      <td>en</td>\n",
       "      <td>7840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>851875102769721344</td>\n",
       "      <td>United</td>\n",
       "      <td>Tue Apr 11 19:10:26 +0000 2017</td>\n",
       "      <td>United CEO Oscar Munoz: I’m sorry. We will fix...</td>\n",
       "      <td>US</td>\n",
       "      <td>-95.8233</td>\n",
       "      <td>29.5223</td>\n",
       "      <td>3098</td>\n",
       "      <td>Fri Mar 04 21:23:11 +0000 2011</td>\n",
       "      <td>913424</td>\n",
       "      <td>46648</td>\n",
       "      <td>en</td>\n",
       "      <td>4774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>851383383888285696</td>\n",
       "      <td>United</td>\n",
       "      <td>Mon Apr 10 10:36:31 +0000 2017</td>\n",
       "      <td>@USAnonymous Flight 3411 from Chicago to Louis...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1373</td>\n",
       "      <td>Fri Mar 04 21:23:11 +0000 2011</td>\n",
       "      <td>913424</td>\n",
       "      <td>46648</td>\n",
       "      <td>en</td>\n",
       "      <td>852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>851947964599648256</td>\n",
       "      <td>Triston</td>\n",
       "      <td>Tue Apr 11 23:59:57 +0000 2017</td>\n",
       "      <td>RT @UnitedOverBooks: now boarding flight 3411 ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2235</td>\n",
       "      <td>Sun Jan 01 02:36:00 +0000 2012</td>\n",
       "      <td>1215</td>\n",
       "      <td>987</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>851947959851921408</td>\n",
       "      <td>TΞddy Rosenthal</td>\n",
       "      <td>Tue Apr 11 23:59:56 +0000 2017</td>\n",
       "      <td>RT @UnitedOverBooks: now boarding flight 3411 ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2235</td>\n",
       "      <td>Thu Jul 09 15:01:43 +0000 2015</td>\n",
       "      <td>80</td>\n",
       "      <td>227</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id             name                      created_at  \\\n",
       "0  851471781827420160           United  Mon Apr 10 16:27:47 +0000 2017   \n",
       "1  851875102769721344           United  Tue Apr 11 19:10:26 +0000 2017   \n",
       "2  851383383888285696           United  Mon Apr 10 10:36:31 +0000 2017   \n",
       "3  851947964599648256          Triston  Tue Apr 11 23:59:57 +0000 2017   \n",
       "4  851947959851921408  TΞddy Rosenthal  Tue Apr 11 23:59:56 +0000 2017   \n",
       "\n",
       "                                                text country_code     long  \\\n",
       "0  United CEO response to United Express Flight 3...           US -95.8233   \n",
       "1  United CEO Oscar Munoz: I’m sorry. We will fix...           US -95.8233   \n",
       "2  @USAnonymous Flight 3411 from Chicago to Louis...                         \n",
       "3  RT @UnitedOverBooks: now boarding flight 3411 ...                         \n",
       "4  RT @UnitedOverBooks: now boarding flight 3411 ...                         \n",
       "\n",
       "      latt  retweet_count                    user_created  followers_count  \\\n",
       "0  29.5223          21757  Fri Mar 04 21:23:11 +0000 2011           913424   \n",
       "1  29.5223           3098  Fri Mar 04 21:23:11 +0000 2011           913424   \n",
       "2                    1373  Fri Mar 04 21:23:11 +0000 2011           913424   \n",
       "3                    2235  Sun Jan 01 02:36:00 +0000 2012             1215   \n",
       "4                    2235  Thu Jul 09 15:01:43 +0000 2015               80   \n",
       "\n",
       "   friends_count user_language  favorite_count  \n",
       "0          46648            en            7840  \n",
       "1          46648            en            4774  \n",
       "2          46648            en             852  \n",
       "3            987            en               0  \n",
       "4            227            en               0  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv('/Users/lisha/Desktop/Final/Data/Data_Clean/Tweets_withINFO.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
